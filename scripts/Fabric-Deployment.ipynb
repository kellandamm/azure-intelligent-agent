# =====================================================================
# MASTER FABRIC DEPLOYMENT NOTEBOOK
# =====================================================================
# Purpose: Provision and orchestrate the full demo architecture in Fabric
# Scope:
#   - Items: Lakehouses (Bronze/Silver/Gold), Eventhouse (optional), Eventstream
#   - Data: Bronze shortcuts to Mirrored DB, Medallion transforms, Gold analytics
#   - Agents: SalesIntelligenceAgent, CustomerSuccessAgent
#   - Pipeline: Orchestration for daily runs
# =====================================================================

# =========================
# 0) CONFIGURATION
# =========================
import os, json, time, uuid, requests
from datetime import datetime, timedelta
from notebookutils import mssparkutils
from pyspark.sql import functions as F
from pyspark.sql.window import Window

# REQUIRED: Update these to match your environment --------------------------------
FABRIC_WORKSPACE_ID   = "<YOUR_WORKSPACE_ID>"
TENANT_ID             = "<YOUR_TENANT_ID>"  # Used for auth header audience
LAKEHOUSE_BRONZE_NAME = "BronzeLakehouse"
LAKEHOUSE_SILVER_NAME = "SilverLakehouse"
LAKEHOUSE_GOLD_NAME   = "GoldLakehouse"
EVENTHOUSE_NAME       = "RetailAnalyticsEventhouse"   # optional
EVENTSTREAM_NAME      = "RetailRealTimeStream"        # optional
PIPELINE_NAME         = "EndToEnd_Medallion_Pipeline"
MIRRORED_DB_ITEM_NAME = "ProductionDB_Mirror"         # exact name of the mirrored Azure SQL item
# The mirrored tables you mirrored from Azure SQL (as created earlier)
MIRRORED_TABLES = [
    "dbo_Categories",
    "dbo_Products",
    "dbo_Customers",
    "dbo_Orders",
    "dbo_OrderItems",
    # new sales/CRM tables (if mirrored too)
    "dbo_Opportunities",
    "dbo_CustomerInteractions",
    "dbo_SupportTickets",
    "dbo_CustomerHealthMetrics",
    "dbo_SalesTargets"
]

# Agents
AGENT_SALES_NAME      = "SalesIntelligenceAgent"
AGENT_CS_NAME         = "CustomerSuccessAgent"
AGENT_DESCRIPTION     = "Fabric data agent for conversational Q&A over curated Gold analytics."

# Security / PII
PRIVILEGED_USERS = ["admin@contoso.com","datagovernance@contoso.com"]  # can view PII in Silver

# Schedule (UTC)
PIPELINE_SCHEDULE_TIME_UTC = "06:00"  # daily, 06:00 UTC

# ---------------------------------------------------------------------------------
# Helper: Acquire AAD token for Fabric REST (uses Fabric runtime identity)
# ---------------------------------------------------------------------------------
def get_fabric_token():
    # Fabric notebooks provide a managed identity token for Power BI/Fabric API via this scope
    # audience: https://analysis.windows.net/powerbi/api
    from synapse.ml.mlflow import get_mlflow_env_config
    cfg = get_mlflow_env_config()
    return cfg.driver_aad_token

FABRIC_API = "https://api.fabric.microsoft.com/v1"
AUTH_HEADER = lambda: {"Authorization": f"Bearer {get_fabric_token()}"}

# ---------------------------------------------------------------------------------
# Helper: Get item by name and type, else create
# ---------------------------------------------------------------------------------
def ensure_item(item_type, name, definition=None):
    # item_type examples: "Lakehouse","Eventhouse","Eventstream","DataPipeline","DataAgent"
    # 1) Try find
    url = f"{FABRIC_API}/workspaces/{FABRIC_WORKSPACE_ID}/items?itemType={item_type}"
    r = requests.get(url, headers=AUTH_HEADER())
    r.raise_for_status()
    items = r.json().get("value", [])
    for it in items:
        if it["displayName"] == name:
            return it["id"]
    # 2) Create
    body = {"displayName": name, "type": item_type}
    if definition:
        body.update({"definition": definition})
    r = requests.post(f"{FABRIC_API}/workspaces/{FABRIC_WORKSPACE_ID}/items",
                      headers={**AUTH_HEADER(), "Content-Type":"application/json"},
                      data=json.dumps(body))
    r.raise_for_status()
    return r.json()["id"]

# ---------------------------------------------------------------------------------
# Helper: Find existing item id by name/type
# ---------------------------------------------------------------------------------
def get_item_id(item_type, name):
    url = f"{FABRIC_API}/workspaces/{FABRIC_WORKSPACE_ID}/items?itemType={item_type}"
    r = requests.get(url, headers=AUTH_HEADER())
    r.raise_for_status()
    for it in r.json().get("value", []):
        if it["displayName"] == name:
            return it["id"]
    return None

# ---------------------------------------------------------------------------------
# 1) Ensure Lakehouses (Bronze/Silver/Gold)
# ---------------------------------------------------------------------------------
bronze_id = ensure_item("Lakehouse", LAKEHOUSE_BRONZE_NAME)
silver_id = ensure_item("Lakehouse", LAKEHOUSE_SILVER_NAME)
gold_id   = ensure_item("Lakehouse", LAKEHOUSE_GOLD_NAME)

print(f"✓ Lakehouses ready | Bronze:{bronze_id} | Silver:{silver_id} | Gold:{gold_id}")

# ---------------------------------------------------------------------------------
# 2) Attach the Bronze lakehouse to this notebook session
# ---------------------------------------------------------------------------------
# Use Spark relative paths for Files/Tables
mssparkutils.lakehouse.configure(workspaceId=FABRIC_WORKSPACE_ID, lakehouseId=bronze_id, alias="bronze")
mssparkutils.lakehouse.configure(workspaceId=FABRIC_WORKSPACE_ID, lakehouseId=silver_id, alias="silver")
mssparkutils.lakehouse.configure(workspaceId=FABRIC_WORKSPACE_ID, lakehouseId=gold_id, alias="gold")
print("✓ Notebook attached to Bronze/Silver/Gold lakehouses")

# ---------------------------------------------------------------------------------
# 3) Create OneLake Shortcuts in Bronze to mirrored Azure SQL tables
# ---------------------------------------------------------------------------------
def create_shortcut(lakehouse_id, shortcut_name, source_item_name, source_path_tables):
    # This uses the Files/Tables shortcut API
    # Shortcut from Mirrored DB Tables into Lakehouse Tables area
    # POST /items/{lakehouseId}/lakes/shortcuts
    src_id = get_item_id("MirroredDatabase", source_item_name)
    if not src_id:
        raise ValueError(f"Mirrored item '{source_item_name}' not found in workspace")

    url = f"{FABRIC_API}/items/{lakehouse_id}/lakes/shortcuts"
    # Destination = Tables/<name>, Source = OneLake path of mirrored table
    for table in source_path_tables:
        body = {
          "name": table,  # keep table name same
          "target": { "path": f"Tables/{table}" },
          "source": {
            "type": "OneLake",
            "itemId": src_id,
            "path": f"Tables/{table}"
          }
        }
        r = requests.post(url, headers={**AUTH_HEADER(),"Content-Type":"application/json"}, data=json.dumps(body))
        # If already exists, ignore
        if r.status_code not in (200,201):
            if "already exists" in r.text.lower():
                print(f"  • Shortcut exists: {table}")
            else:
                r.raise_for_status()
        else:
            print(f"  • Shortcut created: {table}")

print("Creating Bronze shortcuts to Mirrored DB tables...")
create_shortcut(bronze_id, "mirror_shortcuts", MIRRORED_DB_ITEM_NAME, MIRRORED_TABLES)
print("✓ Bronze shortcuts ready")

# ---------------------------------------------------------------------------------
# 4) Bronze → Silver Transformations (aligned schema with PII masking)
# ---------------------------------------------------------------------------------
# Attach Silver lakehouse as default for writes
mssparkutils.lakehouse.setDefault("silver")

current_user_row = spark.sql("select current_user() as u").collect()[0]
current_user = current_user_row["u"]
has_pii = current_user in PRIVILEGED_USERS

# Read Bronze (shortcuts)
b = "bronze."
bronze_categories = spark.read.table(f"{b}dbo_Categories")
bronze_products   = spark.read.table(f"{b}dbo_Products")
bronze_customers  = spark.read.table(f"{b}dbo_Customers")
bronze_orders     = spark.read.table(f"{b}dbo_Orders")
bronze_items      = spark.read.table(f"{b}dbo_OrderItems")

# Optional CRM tables if mirrored
def table_exists(tbl):
    try:
        spark.read.table(f"{b}{tbl}").limit(1).collect()
        return True
    except:
        return False

has_opps  = table_exists("dbo_Opportunities")
has_ints  = table_exists("dbo_CustomerInteractions")
has_tix   = table_exists("dbo_SupportTickets")
has_health= table_exists("dbo_CustomerHealthMetrics")
has_targets=table_exists("dbo_SalesTargets")

# Silver: Categories
silver_categories = (bronze_categories
    .dropDuplicates(["CategoryID"])
    .withColumn("CategoryName", F.trim(F.col("CategoryName")))
    .withColumn("Description", F.trim(F.col("Description")))
    .withColumn("processed_timestamp", F.current_timestamp())
)
silver_categories.write.format("delta").mode("overwrite").option("overwriteSchema","true").saveAsTable("silver_categories")

# Silver: Products
silver_products = (bronze_products
    .join(silver_categories.select("CategoryID", F.col("CategoryName").alias("category_name")), "CategoryID","left")
    .dropDuplicates(["ProductID"])
    .filter(F.col("IsActive")==True)
    .filter(F.col("Price")>0)
    .withColumn("ProductName", F.trim(F.col("ProductName")))
    .withColumn("SKU", F.upper(F.trim(F.col("SKU"))))
    .withColumn("Price", F.col("Price").cast("decimal(10,2)"))
    .withColumn("processed_timestamp", F.current_timestamp())
    .select("ProductID","ProductName","CategoryID",F.col("category_name").alias("CategoryName"),
            "Price","StockQuantity","Description","SKU","IsActive","CreatedDate","ModifiedDate","processed_timestamp")
)
silver_products.write.format("delta").mode("overwrite").option("overwriteSchema","true").saveAsTable("silver_products")

# Silver: Customers (PII masking if not privileged)
if has_pii:
    masked_email = F.lower(F.trim(F.col("Email")))
    masked_phone = F.regexp_replace(F.col("PhoneNumber"), "[^0-9]", "")
    addr = F.col("Address")
else:
    masked_email = F.concat(F.substring(F.col("Email"),1,2), F.lit("***@***"),
                            F.substring(F.regexp_replace(F.col("Email"),".*@",""), -3,3))
    masked_phone = F.concat(F.lit("XXX-XXX-"), F.substring(F.regexp_replace(F.col("PhoneNumber"),"[^0-9]",""), -4,4))
    addr = F.lit("[REDACTED]")

silver_customers = (bronze_customers
    .dropDuplicates(["CustomerID"])
    .withColumn("Email", masked_email)
    .withColumn("PhoneNumber", masked_phone)
    .withColumn("Address", addr)
    .withColumn("FirstName", F.trim(F.col("FirstName")))
    .withColumn("LastName", F.trim(F.col("LastName")))
    .withColumn("City", F.trim(F.col("City")))
    .withColumn("State", F.upper(F.trim(F.col("State"))))
    .withColumn("ZipCode", F.trim(F.col("ZipCode")))
    .withColumn("_pii_masked", F.lit(not has_pii))
    .withColumn("processed_timestamp", F.current_timestamp())
    .select("CustomerID","FirstName","LastName","Email","PhoneNumber","Address","City","State",
            "ZipCode","Country","CustomerSince","IsActive","CreatedDate","ModifiedDate",
            "_pii_masked","processed_timestamp")
)
silver_customers.write.format("delta").mode("overwrite").option("overwriteSchema","true").saveAsTable("silver_customers")

# Silver: Orders
silver_orders = (bronze_orders
    .dropDuplicates(["OrderID"])
    .withColumn("OrderDate", F.to_date(F.col("OrderDate")))
    .withColumn("ShippedDate", F.to_date(F.col("ShippedDate")))
    .withColumn("days_to_ship", F.when(F.col("ShippedDate").isNotNull(), F.datediff(F.col("ShippedDate"),F.col("OrderDate"))))
    .withColumn("TotalAmount", F.col("TotalAmount").cast("decimal(12,2)"))
    .withColumn("processed_timestamp", F.current_timestamp())
    .select("OrderID","CustomerID","OrderDate","ShippedDate","OrderStatus","TotalAmount",
            "ShippingAddress","ShippingCity","ShippingState","ShippingZipCode","ShippingCountry","PaymentMethod",
            "days_to_ship","CreatedDate","ModifiedDate","processed_timestamp")
)
silver_orders.write.format("delta").mode("overwrite").option("overwriteSchema","true").saveAsTable("silver_orders")

# Silver: OrderItems
silver_order_items = (bronze_items
    .dropDuplicates(["OrderItemID"])
    .withColumn("UnitPrice", F.col("UnitPrice").cast("decimal(10,2)"))
    .withColumn("Discount", F.col("Discount").cast("decimal(5,2)"))
    .withColumn("LineTotal", F.col("LineTotal").cast("decimal(12,2)"))
    .withColumn("processed_timestamp", F.current_timestamp())
    .select("OrderItemID","OrderID","ProductID","Quantity","UnitPrice","Discount","LineTotal",
            "CreatedDate","ModifiedDate","processed_timestamp")
)
silver_order_items.write.format("delta").mode("overwrite").option("overwriteSchema","true").saveAsTable("silver_order_items")

# Optional: Silver versions of CRM tables
if has_opps:
    spark.read.table(f"{b}dbo_Opportunities").write.format("delta").mode("overwrite").saveAsTable("silver_opportunities")
if has_ints:
    spark.read.table(f"{b}dbo_CustomerInteractions").write.format("delta").mode("overwrite").saveAsTable("silver_customer_interactions")
if has_tix:
    spark.read.table(f"{b}dbo_SupportTickets").write.format("delta").mode("overwrite").saveAsTable("silver_support_tickets")
if has_health:
    spark.read.table(f"{b}dbo_CustomerHealthMetrics").write.format("delta").mode("overwrite").saveAsTable("silver_customer_health_metrics")
if has_targets:
    spark.read.table(f"{b}dbo_SalesTargets").write.format("delta").mode("overwrite").saveAsTable("silver_sales_targets")

print("✓ Silver layer built")

# ---------------------------------------------------------------------------------
# 5) Silver → Gold Analytics (core + sales/CRM)
# ---------------------------------------------------------------------------------
mssparkutils.lakehouse.setDefault("gold")

categories = spark.read.table("silver_categories")
products   = spark.read.table("silver_products")
customers  = spark.read.table("silver_customers")
orders     = spark.read.table("silver_orders").filter(F.col("OrderStatus")!="Cancelled")
items      = spark.read.table("silver_order_items")

# Gold: Customer360
customer_360 = (orders
 .join(customers,"CustomerID")
 .groupBy("CustomerID","FirstName","LastName","Email","City","State","Country","CustomerSince")
 .agg(F.count("OrderID").alias("total_orders"),
      F.sum("TotalAmount").alias("lifetime_value"),
      F.avg("TotalAmount").alias("avg_order_value"),
      F.max("OrderDate").alias("last_order_date"),
      F.min("OrderDate").alias("first_order_date"),
      F.avg("days_to_ship").alias("avg_delivery_days"))
 .withColumn("customer_tenure_days", F.datediff(F.current_date(),F.col("CustomerSince")))
 .withColumn("recency_days", F.datediff(F.current_date(),F.col("last_order_date")))
 .withColumn("customer_segment",
    F.when(F.col("lifetime_value")>=10000,"VIP")
     .when(F.col("lifetime_value")>=5000,"High Value")
     .when(F.col("lifetime_value")>=1000,"Medium Value")
     .otherwise("Standard"))
 .withColumn("customer_status",
    F.when(F.col("recency_days")<=30,"Active")
     .when(F.col("recency_days")<=90,"At Risk")
     .otherwise("Inactive"))
)
customer_360.write.format("delta").mode("overwrite").saveAsTable("gold_customer_360")

# Gold: Product Performance
prod_perf = (items
 .join(products,"ProductID")
 .join(orders.select("OrderID","OrderDate"),"OrderID")
 .groupBy("ProductID","ProductName","CategoryID","CategoryName","Price","SKU")
 .agg(F.sum("Quantity").alias("total_units_sold"),
      F.count("OrderItemID").alias("times_ordered"),
      F.sum("LineTotal").alias("total_revenue"))
)
w = Window.orderBy(F.col("total_revenue").desc())
prod_perf = prod_perf.withColumn("revenue_rank", F.dense_rank().over(w))
prod_perf.write.format("delta").mode("overwrite").saveAsTable("gold_product_performance")

# Gold: Sales by Category
sales_by_cat = (items
 .join(products.select("ProductID","CategoryID","CategoryName"),"ProductID")
 .join(orders.select("OrderID","OrderDate"),"OrderID")
 .groupBy("CategoryID","CategoryName")
 .agg(F.sum("LineTotal").alias("total_revenue"),
      F.count("OrderItemID").alias("transaction_count"),
      F.sum("Quantity").alias("total_units_sold"),
      F.avg("LineTotal").alias("avg_transaction_value"))
 .orderBy(F.col("total_revenue").desc())
)
sales_by_cat.write.format("delta").mode("overwrite").saveAsTable("gold_sales_by_category")

# Gold: Sales time series
sales_ts = (orders
 .withColumn("year", F.year("OrderDate"))
 .withColumn("quarter", F.quarter("OrderDate"))
 .withColumn("month", F.month("OrderDate"))
 .withColumn("month_name", F.date_format("OrderDate","MMMM"))
 .withColumn("day_of_week", F.dayofweek("OrderDate"))
 .groupBy("OrderDate","year","quarter","month","month_name","day_of_week")
 .agg(F.count("OrderID").alias("daily_orders"),
      F.sum("TotalAmount").alias("daily_revenue"),
      F.avg("TotalAmount").alias("avg_order_value"),
      F.countDistinct("CustomerID").alias("unique_customers"))
 .orderBy("OrderDate")
)
sales_ts.write.format("delta").mode("overwrite").saveAsTable("gold_sales_time_series")

# Optional CRM golds if input present (hot leads, at-risk, etc.)
if has_opps:
    opps = spark.read.table("silver_opportunities")
    ints = spark.read.table("silver_customer_interactions") if has_ints else None
    # Pipeline summary
    pipeline = (opps.filter(F.col("Stage").isin("Lead","Qualification","Proposal","Negotiation"))
        .groupBy("Stage","AssignedTo")
        .agg(F.count("OpportunityID").alias("opportunity_count"),
             F.sum("EstimatedValue").alias("pipeline_value"),
             F.avg("ProbabilityPercent").alias("avg_probability"),
             F.sum(F.col("EstimatedValue")*F.col("ProbabilityPercent")/100).alias("weighted_pipeline_value")))
    pipeline.write.format("delta").mode("overwrite").saveAsTable("gold_sales_pipeline")

    # Hot leads (requires interactions to score fully; fallback if not)
    base_hot = (opps.filter(F.col("Stage").isin("Lead","Qualification","Proposal"))
        .join(customers.select("CustomerID","State"),"CustomerID"))
    if has_ints:
        inter = (ints.groupBy("CustomerID")
            .agg(F.count("*").alias("interaction_count"),
                 F.max("InteractionDate").alias("last_interaction_date")))
        base_hot = base_hot.join(inter,"CustomerID","left").withColumn(
            "days_since_interaction", F.datediff(F.current_date(),F.col("last_interaction_date")))
    else:
        base_hot = base_hot.withColumn("interaction_count", F.lit(0)).withColumn("last_interaction_date",F.lit(None)).withColumn("days_since_interaction",F.lit(999))

    hot = (base_hot
        .withColumn("lead_score",
            (F.col("ProbabilityPercent")*0.4) +
            F.when(F.col("interaction_count")>5, F.lit(30)).otherwise(F.col("interaction_count")*5) +
            F.when(F.col("days_since_interaction")<=7, F.lit(30)).when(F.col("days_since_interaction")<=14, F.lit(20)).otherwise(F.lit(10))
        )
        .withColumn("priority",
            F.when(F.col("lead_score")>=80,"Urgent")
             .when(F.col("lead_score")>=60,"High")
             .when(F.col("lead_score")>=40,"Medium").otherwise("Low"))
        .select("OpportunityID","OpportunityName","CustomerID","Stage","EstimatedValue",
                "ExpectedCloseDate","AssignedTo","lead_score","priority","interaction_count","days_since_interaction")
        .orderBy(F.col("lead_score").desc()))
    hot.write.format("delta").mode("overwrite").saveAsTable("gold_hot_leads")

if has_health and has_tix:
    health = spark.read.table("silver_customer_health_metrics")
    tix = spark.read.table("silver_support_tickets")
    at_risk = (customers
        .join(health,"CustomerID")
        .join(orders.groupBy("CustomerID").agg(F.max("OrderDate").alias("last_purchase_date"),
                                               F.count("*").alias("total_orders"),
                                               F.sum("TotalAmount").alias("lifetime_value")),"CustomerID")
        .join(tix.filter(F.col("Status")!="Closed").groupBy("CustomerID").agg(F.count("*").alias("open_tickets")),
              "CustomerID","left")
        .withColumn("days_since_purchase", F.datediff(F.current_date(),F.col("last_purchase_date")))
        .withColumn("churn_probability",
            F.when(F.col("RenewalRisk")=="High",80).when(F.col("RenewalRisk")=="Medium",50).otherwise(20))
        .withColumn("action_required",
            F.when(F.col("days_since_purchase")>90,"Immediate Contact")
             .when(F.col("open_tickets")>0,"Resolve Support Issues").otherwise("Schedule Check-in"))
        .select("CustomerID","FirstName","LastName","Email","State","HealthScore",
                "RenewalRisk","ContractEndDate","last_purchase_date","days_since_purchase","open_tickets",
                "lifetime_value","churn_probability","action_required")
        .orderBy(F.col("churn_probability").desc()))
    at_risk.write.format("delta").mode("overwrite").saveAsTable("gold_at_risk_customers")

print("✓ Gold layer built")

# ---------------------------------------------------------------------------------
# 6) Eventstream + Eventhouse (optional) for real-time
# ---------------------------------------------------------------------------------
# Create Eventhouse
eventhouse_id = ensure_item("Eventhouse", EVENTHOUSE_NAME)
print(f"✓ Eventhouse ready: {eventhouse_id}")

# Create Eventstream
eventstream_id = ensure_item("Eventstream", EVENTSTREAM_NAME)
print(f"✓ Eventstream ready: {eventstream_id}")

# Add destinations: Lakehouse (Gold) and Eventhouse
# Note: Minimal example for destinations; adjust per your routing/transform needs
def add_eventstream_destination(eventstream_id, dest_name, dest_type, config):
    url = f"{FABRIC_API}/items/{eventstream_id}/eventstreams/destinations"
    body = {"displayName": dest_name, "type": dest_type, "configuration": config}
    r = requests.post(url, headers={**AUTH_HEADER(),"Content-Type":"application/json"}, data=json.dumps(body))
    if r.status_code not in (200,201):
        if "already exists" not in r.text.lower():
            r.raise_for_status()

# Destination 1: Lakehouse Gold delta table 'streaming_transactions'
add_eventstream_destination(
    eventstream_id,
    "LakehouseGold",
    "Lakehouse",
    {
      "workspaceId": FABRIC_WORKSPACE_ID,
      "lakehouseId": gold_id,
      "table": "streaming_transactions",
      "inputFormat": "json",
      "batching": { "maxRows": 500, "maxIntervalSeconds": 120 }
    }
)

# Destination 2: Eventhouse KQL table 'RealTimeTransactions'
add_eventstream_destination(
    eventstream_id,
    "EventhouseKQL",
    "Eventhouse",
    {
      "workspaceId": FABRIC_WORKSPACE_ID,
      "eventhouseId": eventhouse_id,
      "database": EVENTHOUSE_NAME,
      "table": "RealTimeTransactions",
      "inputFormat": "json"
    }
)

print("✓ Eventstream destinations configured")

# ---------------------------------------------------------------------------------
# 7) Fabric Data Agents (Sales & Customer Success)
# ---------------------------------------------------------------------------------
def create_or_update_data_agent(name, description, sources):
    # Create
    agent_id = get_item_id("DataAgent", name)
    if not agent_id:
        agent_id = ensure_item("DataAgent", name)
    # Bind data sources
    url = f"{FABRIC_API}/items/{agent_id}/dataagent/datasources"
    r = requests.put(url, headers={**AUTH_HEADER(),"Content-Type":"application/json"},
                     data=json.dumps({"tables": sources}))
    if r.status_code not in (200,204):
        r.raise_for_status()
    # Publish agent (to get URL)
    pub = requests.post(f"{FABRIC_API}/items/{agent_id}/dataagent/publish", headers=AUTH_HEADER())
    if pub.status_code not in (200,202):
        pub.raise_for_status()
    # Read settings (retrieve published URL)
    s = requests.get(f"{FABRIC_API}/items/{agent_id}/dataagent/settings", headers=AUTH_HEADER())
    s.raise_for_status()
    agent_url = s.json().get("publishedUrl", "")
    return agent_id, agent_url

# Sales agent uses core sales golds
sales_sources = [
    {"workspaceId": FABRIC_WORKSPACE_ID, "itemId": gold_id, "tableName":"gold_sales_pipeline"},
    {"workspaceId": FABRIC_WORKSPACE_ID, "itemId": gold_id, "tableName":"gold_hot_leads"},
    {"workspaceId": FABRIC_WORKSPACE_ID, "itemId": gold_id, "tableName":"gold_sales_performance"},  # created later by KPI notebook if desired
    {"workspaceId": FABRIC_WORKSPACE_ID, "itemId": gold_id, "tableName":"gold_sales_by_category"},
    {"workspaceId": FABRIC_WORKSPACE_ID, "itemId": gold_id, "tableName":"gold_product_performance"},
    {"workspaceId": FABRIC_WORKSPACE_ID, "itemId": gold_id, "tableName":"gold_sales_time_series"}
]
sales_agent_id, sales_agent_url = create_or_update_data_agent(AGENT_SALES_NAME, AGENT_DESCRIPTION, sales_sources)
print(f"✓ Sales Data Agent published: {sales_agent_url}")

# Customer success agent uses health/at-risk/upsell
cs_sources = [
    {"workspaceId": FABRIC_WORKSPACE_ID, "itemId": gold_id, "tableName":"gold_customer_360"},
    {"workspaceId": FABRIC_WORKSPACE_ID, "itemId": gold_id, "tableName":"gold_at_risk_customers"},
    # add more if present, e.g., gold_upsell_opportunities, gold_customer_interactions_summary, gold_support_metrics
]
cs_agent_id, cs_agent_url = create_or_update_data_agent(AGENT_CS_NAME, AGENT_DESCRIPTION, cs_sources)
print(f"✓ Customer Success Data Agent published: {cs_agent_url}")

# ---------------------------------------------------------------------------------
# 8) Data Pipeline (orchestration)
# ---------------------------------------------------------------------------------
# The pipeline runs:
#   Activity 1: Notebook - Bronze→Silver transforms (this notebook can call a child)
#   Activity 2: Notebook - Silver→Gold analytics
#   Activity 3: (Optional) Notebook - Real-time seed generator (if used)
#   Activity 4: Refresh SQL analytics endpoints / Semantic model (optional)

pipeline_id = ensure_item("DataPipeline", PIPELINE_NAME)

# Define basic pipeline JSON with two Notebook activities (by reference to this notebook name or separate names)
# If you prefer separating into child notebooks, create them and reference their item IDs.
pipeline_definition = {
  "activities":[
    {
      "name": "Bronze_to_Silver",
      "type": "Notebook",
      "properties": {
        "notebook": {"inlineCode": "# Bronze→Silver steps executed in master notebook earlier.\n# In a real deployment, split into child notebook and call here."}
      }
    },
    {
      "name": "Silver_to_Gold",
      "type": "Notebook",
      "dependsOn": ["Bronze_to_Silver"],
      "properties": {
        "notebook": {"inlineCode": "# Silver→Gold steps executed in master notebook earlier.\n# In a real deployment, split into child notebook and call here."}
      }
    }
  ],
  "annotations":["medallion","demo","fabric"]
}

# Update pipeline definition
upd = requests.patch(f"{FABRIC_API}/items/{pipeline_id}", headers={**AUTH_HEADER(),"Content-Type":"application/json"},
                     data=json.dumps({"definition": pipeline_definition}))
if upd.status_code not in (200,204):
    upd.raise_for_status()
print(f"✓ Pipeline defined: {PIPELINE_NAME}")

# Schedule (daily)
# POST /schedules
sched_body = {
  "displayName": f"{PIPELINE_NAME}_Daily_6AM",
  "itemId": pipeline_id,
  "recurrence": {
    "pattern": "Daily",
    "timeZone": "UTC",
    "startDateTime": f"{datetime.utcnow().date()}T{PIPELINE_SCHEDULE_TIME_UTC}:00Z",
    "interval": 1
  }
}
# Create or ensure schedule
sch = requests.post(f"{FABRIC_API}/schedules", headers={**AUTH_HEADER(),"Content-Type":"application/json"},
                    data=json.dumps(sched_body))
if sch.status_code not in (200,201):
    if "already exists" not in sch.text.lower():
        sch.raise_for_status()
print("✓ Pipeline schedule set to daily at", PIPELINE_SCHEDULE_TIME_UTC, "UTC")

# ---------------------------------------------------------------------------------
# 9) Outputs / Next steps
# ---------------------------------------------------------------------------------
print("\n=== Deployment Summary ===")
print(f"Workspace: {FABRIC_WORKSPACE_ID}")
print(f"Lakehouses  : Bronze={bronze_id}, Silver={silver_id}, Gold={gold_id}")
print(f"Eventhouse  : {eventhouse_id}")
print(f"Eventstream : {eventstream_id}")
print(f"Pipeline    : {pipeline_id} (scheduled daily {PIPELINE_SCHEDULE_TIME_UTC} UTC)")
print(f"Sales Agent : {sales_agent_id} → URL: {sales_agent_url}")
print(f"CS Agent    : {cs_agent_id} → URL: {cs_agent_url}")
print("\nNext:")
print("1) Run a quick validation query on Gold tables (e.g., gold_sales_by_category) to confirm row counts.")
print("2) Build semantic model from Gold tables (Direct Lake) and publish PBI report.")
print("3) If using real-time, point your front-end/event producers at the Eventstream ingress endpoint.")
